apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      smtp_smarthost: 'smtp.example.com:587'
      smtp_from: 'alertmanager@example.com'
      smtp_auth_username: 'alertmanager@example.com'
      smtp_auth_password: '${SMTP_PASSWORD}'

    route:
      group_by: ['alertname', 'cluster', 'cloud_provider', 'environment', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default'
      routes:
        # Critical alerts - immediate notification
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 10s
          repeat_interval: 1h
          continue: true

        # Production alerts
        - match:
            environment: prod
          receiver: 'prod-alerts'
          group_wait: 30s
          repeat_interval: 2h
          routes:
            - match:
                severity: critical
              receiver: 'prod-critical'
              group_wait: 10s

        # AWS specific alerts
        - match:
            cloud_provider: aws
          receiver: 'aws-alerts'
          continue: true

        # OCI specific alerts
        - match:
            cloud_provider: oci
          receiver: 'oci-alerts'
          continue: true

    receivers:
      - name: 'default'
        webhook_configs:
          - url: 'http://alertmanager-webhook:8080/webhook'
            send_resolved: true

      - name: 'critical-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts-critical'
            send_resolved: true
            title: '{{ .Status | toUpper }}{{ if eq .Status "firing" }} - {{ .Alerts.Firing | len }}{{ end }}'
            text: >-
              {{ range .Alerts }}
              *Alert:* {{ .Labels.alertname }}
              *Severity:* {{ .Labels.severity }}
              *Cluster:* {{ .Labels.cluster }}
              *Cloud:* {{ .Labels.cloud_provider }}
              *Description:* {{ .Annotations.description }}
              *Runbook:* {{ .Annotations.runbook_url }}
              {{ end }}
        pagerduty_configs:
          - service_key: '${PAGERDUTY_SERVICE_KEY}'
            send_resolved: true

      - name: 'prod-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts-prod'
            send_resolved: true
            title: '[PROD] {{ .Status | toUpper }}'
            text: >-
              {{ range .Alerts }}
              *Alert:* {{ .Labels.alertname }}
              *Severity:* {{ .Labels.severity }}
              *Cloud:* {{ .Labels.cloud_provider }}
              {{ end }}

      - name: 'prod-critical'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts-prod-critical'
            send_resolved: true
        pagerduty_configs:
          - service_key: '${PAGERDUTY_SERVICE_KEY}'
            send_resolved: true

      - name: 'aws-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts-aws'
            send_resolved: true

      - name: 'oci-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts-oci'
            send_resolved: true

    inhibit_rules:
      # If critical alert is firing, inhibit warning alerts for same alertname
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'cluster']

      # If cluster is down, inhibit all other alerts for that cluster
      - source_match:
          alertname: 'ClusterDown'
        target_match_re:
          alertname: '.*'
        equal: ['cluster']

    templates:
      - '/etc/alertmanager/templates/*.tmpl'
